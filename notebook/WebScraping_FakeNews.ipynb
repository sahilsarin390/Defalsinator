{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15928,"status":"ok","timestamp":1613107471116,"user":{"displayName":"Sheel Patel","photoUrl":"","userId":"14536439277210965241"},"user_tz":-330},"id":"VHxwRYcnDtQC","outputId":"83c155f4-7e98-42ef-f58c-94bac698be25"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting newspaper3k\n","  Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n","Requirement already satisfied: PyYAML>=3.11 in c:\\users\\sheel\\anaconda3\\envs\\tf2.4-gpu\\lib\\site-packages (from newspaper3k) (5.4.1)\n","Requirement already satisfied: beautifulsoup4>=4.4.1 in c:\\users\\sheel\\anaconda3\\envs\\tf2.4-gpu\\lib\\site-packages (from newspaper3k) (4.10.0)\n","Collecting lxml>=3.6.0\n","  Downloading lxml-4.6.3-cp38-cp38-win_amd64.whl (3.5 MB)\n","Collecting feedfinder2>=0.0.4\n","  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n","Requirement already satisfied: requests>=2.10.0 in c:\\users\\sheel\\anaconda3\\envs\\tf2.4-gpu\\lib\\site-packages (from newspaper3k) (2.25.1)\n","Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\sheel\\anaconda3\\envs\\tf2.4-gpu\\lib\\site-packages (from newspaper3k) (2.8.1)\n","Collecting jieba3k>=0.35.1\n","  Downloading jieba3k-0.35.1.zip (7.4 MB)\n","Collecting tldextract>=2.0.1\n","  Downloading tldextract-3.1.2-py2.py3-none-any.whl (87 kB)\n","Requirement already satisfied: Pillow>=3.3.0 in c:\\users\\sheel\\anaconda3\\envs\\tf2.4-gpu\\lib\\site-packages (from newspaper3k) (8.3.1)\n","Requirement already satisfied: nltk>=3.2.1 in c:\\users\\sheel\\anaconda3\\envs\\tf2.4-gpu\\lib\\site-packages (from newspaper3k) (3.6.2)\n","Collecting feedparser>=5.2.1\n","  Downloading feedparser-6.0.8-py3-none-any.whl (81 kB)\n","Collecting tinysegmenter==0.3\n","  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n","Collecting cssselect>=0.9.2\n","  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: soupsieve>1.2 in c:\\users\\sheel\\anaconda3\\envs\\tf2.4-gpu\\lib\\site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.2.1)\n","Requirement already satisfied: six in c:\\users\\sheel\\anaconda3\\envs\\tf2.4-gpu\\lib\\site-packages (from feedfinder2>=0.0.4->newspaper3k) (1.15.0)\n","Collecting sgmllib3k\n","  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n","Requirement already satisfied: tqdm in c:\\users\\sheel\\anaconda3\\envs\\tf2.4-gpu\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (4.61.1)\n","Requirement already satisfied: regex in c:\\users\\sheel\\anaconda3\\envs\\tf2.4-gpu\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (2021.4.4)\n","Requirement already satisfied: click in c:\\users\\sheel\\anaconda3\\envs\\tf2.4-gpu\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (7.1.2)\n","Requirement already satisfied: joblib in c:\\users\\sheel\\anaconda3\\envs\\tf2.4-gpu\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (1.0.1)\n","Requirement already satisfied: idna<3,>=2.5 in c:\\users\\sheel\\anaconda3\\envs\\tf2.4-gpu\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2.10)\n","Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\sheel\\anaconda3\\envs\\tf2.4-gpu\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (4.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sheel\\anaconda3\\envs\\tf2.4-gpu\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2021.5.30)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\sheel\\anaconda3\\envs\\tf2.4-gpu\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (1.26.5)\n","Collecting filelock>=3.0.8\n","  Downloading filelock-3.3.0-py3-none-any.whl (9.7 kB)\n","Collecting requests-file>=1.4\n","  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n","Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n","  Building wheel for tinysegmenter (setup.py): started\n","  Building wheel for tinysegmenter (setup.py): finished with status 'done'\n","  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13537 sha256=c6ea85fbbf121bc376a5c414c32116339b984109da409959b5952e1f8cfae5ab\n","  Stored in directory: c:\\users\\sheel\\appdata\\local\\pip\\cache\\wheels\\99\\74\\83\\8fac1c8d9c648cfabebbbffe97a889f6624817f3aa0bbe6c09\n","  Building wheel for feedfinder2 (setup.py): started\n","  Building wheel for feedfinder2 (setup.py): finished with status 'done'\n","  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3354 sha256=92aef791b9eb723af9334e02cd18e796b9aca4d9439eb8a4a4a860cb5d5a0ec9\n","  Stored in directory: c:\\users\\sheel\\appdata\\local\\pip\\cache\\wheels\\b6\\09\\68\\a9f15498ac02c23dde29f18745bc6a6f574ba4ab41861a3575\n","  Building wheel for jieba3k (setup.py): started\n","  Building wheel for jieba3k (setup.py): finished with status 'done'\n","  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398406 sha256=0b599fc6a9327bbb85afa793d230a98b2c467163f6cd5f4af03c4b118171e1ae\n","  Stored in directory: c:\\users\\sheel\\appdata\\local\\pip\\cache\\wheels\\1f\\7e\\0c\\54f3b0f5164278677899f2db08f2b07943ce2d024a3c862afb\n","  Building wheel for sgmllib3k (setup.py): started\n","  Building wheel for sgmllib3k (setup.py): finished with status 'done'\n","  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6065 sha256=f677783ec342646d653bc01a48b0d50b352bb06b123b0aaf676702721b59ccd5\n","  Stored in directory: c:\\users\\sheel\\appdata\\local\\pip\\cache\\wheels\\83\\63\\2f\\117884c3b19d46b64d3d61690333aa80c88dc14050e269c546\n","Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n","Installing collected packages: sgmllib3k, requests-file, filelock, tldextract, tinysegmenter, lxml, jieba3k, feedparser, feedfinder2, cssselect, newspaper3k\n","Successfully installed cssselect-1.1.0 feedfinder2-0.0.4 feedparser-6.0.8 filelock-3.3.0 jieba3k-0.35.1 lxml-4.6.3 newspaper3k-0.2.8 requests-file-1.5.1 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-3.1.2\n","Requirement already satisfied: feedparser in c:\\users\\sheel\\anaconda3\\envs\\tf2.4-gpu\\lib\\site-packages (6.0.8)\n","Requirement already satisfied: sgmllib3k in c:\\users\\sheel\\anaconda3\\envs\\tf2.4-gpu\\lib\\site-packages (from feedparser) (1.0.0)\n"]}],"source":["!pip3 install newspaper3k\n","!pip3 install feedparser"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26447,"status":"ok","timestamp":1613107397984,"user":{"displayName":"Sheel Patel","photoUrl":"","userId":"14536439277210965241"},"user_tz":-330},"id":"_zZeHGf8C1Ib","outputId":"3d0cfdad-2da7-4867-8b61-748d62d72abe"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/gdrive\n"]}],"source":["import os\n","from google.colab import drive\n","\n","# Mount google drive\n","DRIVE_MOUNT='/content/gdrive'\n","drive.mount(DRIVE_MOUNT)\n","\n","# create folder to write data to\n","Fake_News=os.path.join(DRIVE_MOUNT, 'My Drive', 'Fake News')\n","Project=os.path.join(Fake_News, 'Project')\n","os.makedirs(Project, exist_ok=True)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"o-odhsYQDOf1"},"outputs":[],"source":["import json\n","dictionary = {\n","  \"breitbart\": {\n","    \"link\": \"https://www.breitbart.com/search/?s=crypto#gsc.tab=0&gsc.q=crypto&gsc.page=1\",\n","    \"rss\" : \"https://www.breitbart.com/search/?s=crypto\"\n","  }\n","}"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"Y-owBGAiDbZ8"},"outputs":[],"source":["json_object = json.dumps(dictionary, indent = 4)\n","with open(\"NewsPapers.json\", \"w\") as outfile: \n","    outfile.write(json_object)"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1784745,"status":"ok","timestamp":1613109260447,"user":{"displayName":"Sheel Patel","photoUrl":"","userId":"14536439277210965241"},"user_tz":-330},"id":"UUCz811gDkqu","outputId":"a1a742dd-f795-4a5d-9e86-457128cb1e25"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading articles from  breitbart\n"]}],"source":["import feedparser as fp\n","import json\n","import newspaper\n","from newspaper import Article\n","from time import mktime\n","from datetime import datetime\n","\n","# Set the limit for number of articles to download\n","LIMIT = 14500\n","\n","data = {}\n","data['newspapers'] = {}\n","\n","# Loads the JSON files with news sites\n","with open('NewsPapers.json') as data_file:\n","    companies = json.load(data_file)\n","\n","count = 1\n","\n","# Iterate through each news company\n","for company, value in companies.items():\n","    # If a RSS link is provided in the JSON file, this will be the first choice.\n","    # Reason for this is that, RSS feeds often give more consistent and correct data.\n","    # If you do not want to scrape from the RSS-feed, just leave the RSS attr empty in the JSON file.\n","    if 'rss' in value:\n","        d = fp.parse(value['rss'])\n","        print(\"Downloading articles from \", company)\n","        newsPaper = {\n","            \"rss\": value['rss'],\n","            \"link\": value['link'],\n","            \"articles\": []\n","        }\n","        for entry in d.entries:\n","            # Check if publish date is provided, if no the article is skipped.\n","            # This is done to keep consistency in the data and to keep the script from crashing.\n","            if hasattr(entry, 'published'):\n","                if count > LIMIT:\n","                    break\n","                article = {}\n","                article['link'] = entry.link\n","                date = entry.published_parsed\n","                article['published'] = datetime.fromtimestamp(mktime(date)).isoformat()\n","                try:\n","                    content = Article(entry.link)\n","                    content.download()\n","                    content.parse()\n","                except Exception as e:\n","                    # If the download for some reason fails (ex. 404) the script will continue downloading\n","                    # the next article.\n","                    print(e)\n","                    print(\"continuing...\")\n","                    continue\n","                article['title'] = content.title\n","                article['text'] = content.text\n","                newsPaper['articles'].append(article)\n","                print(count, \"articles downloaded from\", company, \", url: \", entry.link)\n","                count = count + 1\n","    else:\n","        # This is the fallback method if a RSS-feed link is not provided.\n","        # It uses the python newspaper library to extract articles\n","        print(\"Building site for \", company)\n","        paper = newspaper.build(value['link'], memoize_articles=False)\n","        newsPaper = {\n","            \"link\": value['link'],\n","            \"articles\": []\n","        }\n","        noneTypeCount = 0\n","        for content in paper.articles:\n","            if count > LIMIT:\n","                break\n","            try:\n","                content.download()\n","                content.parse()\n","            except Exception as e:\n","                print(e)\n","                print(\"continuing...\")\n","                continue\n","            # Again, for consistency, if there is no found publish date the article will be skipped.\n","            # After 10 downloaded articles from the same newspaper without publish date, the company will be skipped.\n","            if content.publish_date is None:\n","                print(count, \" Article has date of type None...\")\n","                noneTypeCount = noneTypeCount + 1\n","                if noneTypeCount > 100:\n","                    print(\"Too many noneType dates, aborting...\")\n","                    noneTypeCount = 0\n","                    break\n","                count = count + 1\n","                continue\n","            article = {}\n","            article['title'] = content.title\n","            article['text'] = content.text\n","            article['link'] = content.url\n","            article['published'] = content.publish_date.isoformat()\n","            newsPaper['articles'].append(article)\n","            print(count, \"articles downloaded from\", company, \" using newspaper, url: \", content.url)\n","            count = count + 1\n","            noneTypeCount = 0\n","    count = 1\n","    data['newspapers'][company] = newsPaper\n","\n","# Finally it saves the articles as a JSON-file.\n","try:\n","    with open('scraped_articles.json', 'w') as outfile:\n","        json.dump(data, outfile)\n","except Exception as e: print(e)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"oqcRuKkiDlVU"},"outputs":[],"source":["with open('scraped_articles.json') as json_data:\n","    d = json.load(json_data)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":719,"status":"ok","timestamp":1613109311770,"user":{"displayName":"Sheel Patel","photoUrl":"","userId":"14536439277210965241"},"user_tz":-330},"id":"Clt2nmBiEb-A","outputId":"13bfc633-4d23-416b-f62b-d102bed968c0"},"outputs":[{"name":"stdout","output_type":"stream","text":["0 breitbart\n"]}],"source":["for i, site in enumerate((list(d['newspapers']))):\n","    print(i, site)"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"Y01p8E7GEh5u"},"outputs":[],"source":["\n","import pandas as pd\n","for i, site in enumerate((list(d['newspapers']))):\n","    articles = list(d['newspapers'][site]['articles'])\n","    if i == 0:\n","        df = pd.DataFrame.from_dict(articles)\n","        df[\"site\"] = site\n","    else:\n","        new_df = pd.DataFrame.from_dict(articles)\n","        new_df[\"site\"] = site\n","        df = pd.concat([df, new_df], ignore_index = True)"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":921,"status":"ok","timestamp":1613109320021,"user":{"displayName":"Sheel Patel","photoUrl":"","userId":"14536439277210965241"},"user_tz":-330},"id":"VD6XnVGDEiT7","outputId":"4e3ecb7c-4e1d-44aa-d9ad-90531b26393e"},"outputs":[{"data":{"text/plain":["(0, 1)"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["df.shape"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":195},"executionInfo":{"elapsed":966,"status":"ok","timestamp":1613109321724,"user":{"displayName":"Sheel Patel","photoUrl":"","userId":"14536439277210965241"},"user_tz":-330},"id":"ybROkyvXEs9u","outputId":"13b0f979-bce6-41a0-e165-6aeb119d8750"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>site</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["Empty DataFrame\n","Columns: [site]\n","Index: []"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b-A8vZDXEuMC"},"outputs":[],"source":["!cp scraped_articles.csv \"/content/gdrive/My Drive/Fake News/Project/\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tMU-SMUJE1sk"},"outputs":[],"source":["df.to_csv('scraped_articles.csv')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SoNISdyTzvwf"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMKD7OUNiwJ6fRoqtoteotG","name":"WebScraping_FakeNews.ipynb","provenance":[]},"interpreter":{"hash":"915a92bfab1603c3236c0ba9c99ce71e3e393079483f4d139e4fb0945eac66d5"},"kernelspec":{"display_name":"Python 3.8.0 64-bit ('tf2.4-gpu': conda)","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"}},"nbformat":4,"nbformat_minor":0}
